{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview of Tapirs Tapirs is a Snakemake workflow system to reproducibly process metabarcode DNA sequences and assign taxonomy. The Tapirs workflow starts with a directory of demultiplexed fastq.gz sequences. There are three key sections to its workflow: Quality Control quality trimming, length trimming, denoising to remove errors, and dereplication to remove redundancy Taxonomic Assignment Taxonomic identity is assigned to each sequence by a variety of methods including blast with different LCA approaches, SINTAX kmer analysis, and Kraken2. Since we use a workflow manager (Snakemake) methods can easily be added to this list without affecting the rest of the workflow Reports and Graphical Display The workflow will write a detailed report of its analyses and actions, and output to standard format BIOM and .tsv files. Krona is used to create interactive html graphical displays of the data. The Vegan R package is used to calculate diversity statistics and plots. Quickstart install conda (miniconda) git clone the Tapirs repository git clone https://github.com/davelunt/Tapirs install snakemake in your base conda environment conda activate base conda install -c bioconda -c conda-forge snakemake Place all library directories within the \"data/01_demultiplexed/\" directory ensuring they follow the format: data/01_demultiplexed/<library>/<sample>.<read>.fastq.gz Run the script to create the library and sample lists from your data. bash scripts/wildcarding.sh dry run snakemake -s snakefile --use-conda --printshellcmds -n to identify any issues run snakemake -s snakefile --use-conda --printshellcmds See the installation and setup pages for more detailed help Licence and citation Project led by Dave Lunt , Michael Winter, Graham Sellers, Marco Benucci, and the EvoHull group at the University of Hull, UK. The software is released as CC0, public domain, you may do as you wish. Please cite the software like this: Title: Tapirs: extensible reproducible workflows for metabarcoding Authors: doi: 1234567 URL: https://github.com/davelunt/Tapirs Please also cite the software generating the analyses. An appropriate way to do this would be: \"A reproducible metabarcoding workflow was implemented in Tapirs [1] using vsearch [2], blast [3], Kraken2 [4], and Krona [5].\" A graph of a typical Tapirs workflow As part of its report-writing Tapirs will create a DAG illustration of its workflow.","title":"Home"},{"location":"#overview-of-tapirs","text":"Tapirs is a Snakemake workflow system to reproducibly process metabarcode DNA sequences and assign taxonomy. The Tapirs workflow starts with a directory of demultiplexed fastq.gz sequences. There are three key sections to its workflow: Quality Control quality trimming, length trimming, denoising to remove errors, and dereplication to remove redundancy Taxonomic Assignment Taxonomic identity is assigned to each sequence by a variety of methods including blast with different LCA approaches, SINTAX kmer analysis, and Kraken2. Since we use a workflow manager (Snakemake) methods can easily be added to this list without affecting the rest of the workflow Reports and Graphical Display The workflow will write a detailed report of its analyses and actions, and output to standard format BIOM and .tsv files. Krona is used to create interactive html graphical displays of the data. The Vegan R package is used to calculate diversity statistics and plots.","title":"Overview of Tapirs"},{"location":"#quickstart","text":"install conda (miniconda) git clone the Tapirs repository git clone https://github.com/davelunt/Tapirs install snakemake in your base conda environment conda activate base conda install -c bioconda -c conda-forge snakemake Place all library directories within the \"data/01_demultiplexed/\" directory ensuring they follow the format: data/01_demultiplexed/<library>/<sample>.<read>.fastq.gz Run the script to create the library and sample lists from your data. bash scripts/wildcarding.sh dry run snakemake -s snakefile --use-conda --printshellcmds -n to identify any issues run snakemake -s snakefile --use-conda --printshellcmds See the installation and setup pages for more detailed help","title":"Quickstart"},{"location":"#licence-and-citation","text":"Project led by Dave Lunt , Michael Winter, Graham Sellers, Marco Benucci, and the EvoHull group at the University of Hull, UK. The software is released as CC0, public domain, you may do as you wish. Please cite the software like this: Title: Tapirs: extensible reproducible workflows for metabarcoding Authors: doi: 1234567 URL: https://github.com/davelunt/Tapirs Please also cite the software generating the analyses. An appropriate way to do this would be: \"A reproducible metabarcoding workflow was implemented in Tapirs [1] using vsearch [2], blast [3], Kraken2 [4], and Krona [5].\"","title":"Licence and citation"},{"location":"#a-graph-of-a-typical-tapirs-workflow","text":"As part of its report-writing Tapirs will create a DAG illustration of its workflow.","title":"A graph of a typical Tapirs workflow"},{"location":"About/background/","text":"Background We developed Tapirs primarily for our own work in the EvoHull group at the University of Hull, UK. It has been common for us to carry out short (Illumina) metabarcoding on environmental or eDNA community samples. Motivation We wanted a transparent workflow that was not dependent on the person who developed the software, could be understood easily by new people in the lab, and enabled us to take reproducibility seriously. We wanted to be able to vary conditions and re-analyse data experimentally, and that meant that changing parameters and re-running should be easy. We had previously used our own metabarcoding software metaBEAT , written as a large python script. This was very productive but we found that supporting it was increasingly difficult, and adding new analyses to this pipeline is not straightforward. We learned a great deal from designing and using metaBEAT, and it greatly increased our interest in workflow management software. Other software In addition to metaBEAT there are many other wonderful software packages for sophisticated analysis of metabarcoding data. QIIME2 is particularly powerful, as is Mothur, and too many others to mention. We did not feel that other metabarcoding software met our needs for flexibility, reproducibility, and experimentation. We saw a lot value for our research in having a standard workflow manager, with design simplicity, rather than the bigger more powerful but more opaque and fixed packages designed primarily for 16S bacterial sequencing. Using the Snakemake workflow management system We decided that it would be best practice to use a well-designed workflow manager rather than link together our software components in a more ad hoc manner with our own scripts. We chose the Snakemake workflow management system . This is well-designed software, heavily used and tested across a large community of bioinformatics researchers. We explicitly decided to use a specialist workflow manager, rather than designing a pipeline system ourselves, as we valued the ability to modify and experiment with our workflow, and this is by definition what workflow managers do. Reproducibility We take reproducible science very seriously, and so should you. Not only is it the highest quality science, but it is also the easiest science, saving you time and effort. We hope that the way we have designed Tapirs will give others a very good chance to exactly repeat what you did without significant suffering, and also to use their own data with your exact approach to build upon your work. Please see the Reproducibility page for details of maximising the reproducibility of your experiment. We would greatly appreciate your thoughts on better reproducibility in metabarcoding. Contributions Very many people have contributed to Tapirs. Many scientists have worked in EvoHull, discussed metabarcoding software and approaches with us, made suggestions about good practice, explored data analysis, critiqued this work, given feedback and made suggestions on metabarcoding courses, and taught us how to proceed. Although the code here was physically written by Dave Lunt, Michael Winter, Graham Sellers, and Marco Benucci, a much larger pool of people have contributed significantly.","title":"Background"},{"location":"About/background/#background","text":"We developed Tapirs primarily for our own work in the EvoHull group at the University of Hull, UK. It has been common for us to carry out short (Illumina) metabarcoding on environmental or eDNA community samples.","title":"Background"},{"location":"About/background/#motivation","text":"We wanted a transparent workflow that was not dependent on the person who developed the software, could be understood easily by new people in the lab, and enabled us to take reproducibility seriously. We wanted to be able to vary conditions and re-analyse data experimentally, and that meant that changing parameters and re-running should be easy. We had previously used our own metabarcoding software metaBEAT , written as a large python script. This was very productive but we found that supporting it was increasingly difficult, and adding new analyses to this pipeline is not straightforward. We learned a great deal from designing and using metaBEAT, and it greatly increased our interest in workflow management software.","title":"Motivation"},{"location":"About/background/#other-software","text":"In addition to metaBEAT there are many other wonderful software packages for sophisticated analysis of metabarcoding data. QIIME2 is particularly powerful, as is Mothur, and too many others to mention. We did not feel that other metabarcoding software met our needs for flexibility, reproducibility, and experimentation. We saw a lot value for our research in having a standard workflow manager, with design simplicity, rather than the bigger more powerful but more opaque and fixed packages designed primarily for 16S bacterial sequencing.","title":"Other software"},{"location":"About/background/#using-the-snakemake-workflow-management-system","text":"We decided that it would be best practice to use a well-designed workflow manager rather than link together our software components in a more ad hoc manner with our own scripts. We chose the Snakemake workflow management system . This is well-designed software, heavily used and tested across a large community of bioinformatics researchers. We explicitly decided to use a specialist workflow manager, rather than designing a pipeline system ourselves, as we valued the ability to modify and experiment with our workflow, and this is by definition what workflow managers do.","title":"Using the Snakemake workflow management system"},{"location":"About/background/#reproducibility","text":"We take reproducible science very seriously, and so should you. Not only is it the highest quality science, but it is also the easiest science, saving you time and effort. We hope that the way we have designed Tapirs will give others a very good chance to exactly repeat what you did without significant suffering, and also to use their own data with your exact approach to build upon your work. Please see the Reproducibility page for details of maximising the reproducibility of your experiment. We would greatly appreciate your thoughts on better reproducibility in metabarcoding.","title":"Reproducibility"},{"location":"About/background/#contributions","text":"Very many people have contributed to Tapirs. Many scientists have worked in EvoHull, discussed metabarcoding software and approaches with us, made suggestions about good practice, explored data analysis, critiqued this work, given feedback and made suggestions on metabarcoding courses, and taught us how to proceed. Although the code here was physically written by Dave Lunt, Michael Winter, Graham Sellers, and Marco Benucci, a much larger pool of people have contributed significantly.","title":"Contributions"},{"location":"About/design/","text":"DIRECTORY STRUCTURE We set directory structure in the repository to standardise the workflow design. You will need to populate the data directory with your demultiplexed sequence (fastq.gz) files and the databases you wish to search them against (eg blast, kraken2, sintax). Above: the directory structure of a standard Tapirs workflow snakefile The top level snakefile to control the workflow. config file config.yaml is the experiment specific control file. You should edit this to name your experiment, specify the location of data, and edit any variable. envs conda environments for each rule can be specified here. These are in addition to the general top-level environment.yaml and may not be required. data We treat the data directory as read-only during operation of the workflow. It is the location for you to assemble sequences, and databases, and taxonomy, but the workflow will not write here. demultiplexed We assume your raw data has already been demultiplexed and exists as .fastq.gz format files in a directory specified in the config.yaml file. Since de-multiplexing is different in different laboratories we treat this as a separate workflow. databases The databases required will depend on the type of analyses to be carried out. At present we recommend DNA databases for blast, kraken and sintax. You will also need taxonomy information (often called taxdump). documentation /docs contains the documentation for Tapirs, built with mkdocs . rules This contains snakemake workflow description rules for separate tasks. We may have: blast.smk kraken.smk qc.smk report.smk Each of these snakemake rules is run by the main snakefile at the appropriate time. Having separate rules for different sections of the workflow (eg quality control, qc.smk) allows better organisation and simplification of each component within the workflow. In our experience this makes the workflow much more understandable and easier to modify. reports Reports are written by some of the programs. Snakemake will also write an overall report. scripts Scripts called by snakemake rules are placed here. results This directory usually has subdirectories named by program (eg blast). It is a convenient way of organising the output. ANALYSIS fastp fastp is used for 2 separate jobs: quality control, filter reads by score and length merge Forward and Reverse reads. It additionally does error correction if merged reads have a mismatch, taking the highest quality nucleotide vsearch vsearch does several jobs: conversion of fastq.gz to fasta file format dereplication, removal of identical sequences denoising, removal of sequencing errors chimera removal rereplication, tracing consensus sequences back to original reads The sequences written by vsearch are the query sequences for taxonomic identification. blast and MLCA blast blastn is used to search a pre-prepared database for sequence matches. The hits for each query sequence are written including a taxonomic identifier. MLCA MLCA (Majority Lowest Common Ancestor) will determine the LCA of the blast hits for each query sequence. It has several options for customising your blast analysis. Majority means that it differs from other LCA scripts in that it can determine LCA from the majority rather than all the hits. Imagine a situation in which 9/10 high quality blast hits for a sequence are to the brine shrimp Artemia franciscana and one is to the Zebra fish Danio rerio (commonly fed on Artemia in the lab). If the LCA looks for the taxonomy shared by ALL the top 10 hits (including the likely misclassified Danio record) the LCA is Bilateria and almost all information is lost. If however we decide to set the majority parameter -m to 0.8 then the LCA will determine the taxonomy shared by 80% of the top hits, which will be Artemia franciscana. If you set -m to 1.0 then it will require 100% agreement, as other LCAs do. We find this flexibility useful. Kraken2 Kraken2 is an alternative to blast and LCA. It uses a k-mer approach to determine taxonomy of query sequences by comparing to a databases built from reference sequences and taxonomic information. Databases are large, and require significant RAM and time to produce and their construction is not part of this workflow. Kraken2 searches themselves however are very fast and efficient. sintax Sintax assigns taxonomy to query sequences by kmer similarity. graphical display of results Krona is used to make an interactive html page to display taxonomic summaries","title":"Design"},{"location":"About/design/#directory-structure","text":"We set directory structure in the repository to standardise the workflow design. You will need to populate the data directory with your demultiplexed sequence (fastq.gz) files and the databases you wish to search them against (eg blast, kraken2, sintax). Above: the directory structure of a standard Tapirs workflow","title":"DIRECTORY STRUCTURE"},{"location":"About/design/#snakefile","text":"The top level snakefile to control the workflow.","title":"snakefile"},{"location":"About/design/#config-file","text":"config.yaml is the experiment specific control file. You should edit this to name your experiment, specify the location of data, and edit any variable.","title":"config file"},{"location":"About/design/#envs","text":"conda environments for each rule can be specified here. These are in addition to the general top-level environment.yaml and may not be required.","title":"envs"},{"location":"About/design/#data","text":"We treat the data directory as read-only during operation of the workflow. It is the location for you to assemble sequences, and databases, and taxonomy, but the workflow will not write here.","title":"data"},{"location":"About/design/#demultiplexed","text":"We assume your raw data has already been demultiplexed and exists as .fastq.gz format files in a directory specified in the config.yaml file. Since de-multiplexing is different in different laboratories we treat this as a separate workflow.","title":"demultiplexed"},{"location":"About/design/#databases","text":"The databases required will depend on the type of analyses to be carried out. At present we recommend DNA databases for blast, kraken and sintax. You will also need taxonomy information (often called taxdump).","title":"databases"},{"location":"About/design/#documentation","text":"/docs contains the documentation for Tapirs, built with mkdocs .","title":"documentation"},{"location":"About/design/#rules","text":"This contains snakemake workflow description rules for separate tasks. We may have: blast.smk kraken.smk qc.smk report.smk Each of these snakemake rules is run by the main snakefile at the appropriate time. Having separate rules for different sections of the workflow (eg quality control, qc.smk) allows better organisation and simplification of each component within the workflow. In our experience this makes the workflow much more understandable and easier to modify.","title":"rules"},{"location":"About/design/#reports","text":"Reports are written by some of the programs. Snakemake will also write an overall report.","title":"reports"},{"location":"About/design/#scripts","text":"Scripts called by snakemake rules are placed here.","title":"scripts"},{"location":"About/design/#results","text":"This directory usually has subdirectories named by program (eg blast). It is a convenient way of organising the output.","title":"results"},{"location":"About/design/#analysis","text":"","title":"ANALYSIS"},{"location":"About/design/#fastp","text":"fastp is used for 2 separate jobs: quality control, filter reads by score and length merge Forward and Reverse reads. It additionally does error correction if merged reads have a mismatch, taking the highest quality nucleotide","title":"fastp"},{"location":"About/design/#vsearch","text":"vsearch does several jobs: conversion of fastq.gz to fasta file format dereplication, removal of identical sequences denoising, removal of sequencing errors chimera removal rereplication, tracing consensus sequences back to original reads The sequences written by vsearch are the query sequences for taxonomic identification.","title":"vsearch"},{"location":"About/design/#blast-and-mlca","text":"","title":"blast and MLCA"},{"location":"About/design/#blast","text":"blastn is used to search a pre-prepared database for sequence matches. The hits for each query sequence are written including a taxonomic identifier.","title":"blast"},{"location":"About/design/#mlca","text":"MLCA (Majority Lowest Common Ancestor) will determine the LCA of the blast hits for each query sequence. It has several options for customising your blast analysis. Majority means that it differs from other LCA scripts in that it can determine LCA from the majority rather than all the hits. Imagine a situation in which 9/10 high quality blast hits for a sequence are to the brine shrimp Artemia franciscana and one is to the Zebra fish Danio rerio (commonly fed on Artemia in the lab). If the LCA looks for the taxonomy shared by ALL the top 10 hits (including the likely misclassified Danio record) the LCA is Bilateria and almost all information is lost. If however we decide to set the majority parameter -m to 0.8 then the LCA will determine the taxonomy shared by 80% of the top hits, which will be Artemia franciscana. If you set -m to 1.0 then it will require 100% agreement, as other LCAs do. We find this flexibility useful.","title":"MLCA"},{"location":"About/design/#kraken2","text":"Kraken2 is an alternative to blast and LCA. It uses a k-mer approach to determine taxonomy of query sequences by comparing to a databases built from reference sequences and taxonomic information. Databases are large, and require significant RAM and time to produce and their construction is not part of this workflow. Kraken2 searches themselves however are very fast and efficient.","title":"Kraken2"},{"location":"About/design/#sintax","text":"Sintax assigns taxonomy to query sequences by kmer similarity.","title":"sintax"},{"location":"About/design/#graphical-display-of-results","text":"Krona is used to make an interactive html page to display taxonomic summaries","title":"graphical display of results"},{"location":"About/faqs/","text":"Frequently Asked Questions Why is it called Tapirs? What does it stand for? Tapirs is not an acronym, we just called it Tapirs because an easy name helps everyone, and tapirs are cool. Why did you make this and not use X Nothing met our needs, see the background page for discussion. We wanted an extensible workflow, built by a specialist workflow manager, that would allow real reproducibility. Can you add in my favourite software for me? No, sorry. We really hope however that choosing a workflow manager (Snakemake) means that this is much easier than for any other metabarcoding package. Look at the extending Tapirs How-To guide . I've added some tools, do you want my changes? Yes please. Do this via a pull request on github. There is an error/hole in the documentation You should just be able to edit and fix it on Github. Open the page and click the pencil icon, top right, to edit. Thanks for your help. If you think that parts of the documentation are lacking, and you would like to significantly extend them, we also welcome that. If you prefer to discuss it before you start then please do get in touch. It works on one machine but not on another Run-anywhere portability is a real challenge in bioinformatics, not just for Tapirs. Tapirs has been tested succesfully on Windows, OSX, and Linux, but problems may still arise. Conda environments have proved really useful for us, but they aren't perfect. As a start just check that both machines are running the most recent version of Conda and you have the most recent version of Tapirs. Make sure Conda is activated to the base environment, has snakemake installed (check with conda list ), and you are using the --use-conda flag in your initial snakemake command. You could also take the very detailed exported environment (reports/archived_envs) from the successful machine and use it to create a new environment on the not-running machine ( conda env create --file environment.yaml ). Activate this environment and do not use the --use-conda flag when running the initial snakemake command. Both machines should then have exactly the same software installed. Who do I contact about the software? The best way is to flag this on GitHub. If you need to get in touch personally then contact Dave Lunt on Gmail. Is it licensed? No, all parts that we have written are free under CC0 public domain and you may do as you wish. Yes, you may include whatever you wish into your own software, we encourage this. All software components that we have included (rather than written, see environment.yaml) have been chosen because they have permissive open source licences, but you may wish to check the details for yourself. Is there a citation We greatly appreciate you citing us wherever you are able. A suitable citation might be: Title: Tapirs, an extensible workflow for reproducible metabarcoding Authors: URL: https://github.com/davelunt/Tapirs An appropriate way to do this would be to include references to the original analysis software: \"A reproducible metabarcoding workflow was implemented in Tapirs [1] using blast [2], Kraken2 [3], SINTAX [4] and Krona [5].\"","title":"FAQs"},{"location":"About/faqs/#why-is-it-called-tapirs-what-does-it-stand-for","text":"Tapirs is not an acronym, we just called it Tapirs because an easy name helps everyone, and tapirs are cool.","title":"Why is it called Tapirs? What does it stand for?"},{"location":"About/faqs/#why-did-you-make-this-and-not-use-x","text":"Nothing met our needs, see the background page for discussion. We wanted an extensible workflow, built by a specialist workflow manager, that would allow real reproducibility.","title":"Why did you make this and not use X"},{"location":"About/faqs/#can-you-add-in-my-favourite-software-for-me","text":"No, sorry. We really hope however that choosing a workflow manager (Snakemake) means that this is much easier than for any other metabarcoding package. Look at the extending Tapirs How-To guide .","title":"Can you add in my favourite software for me?"},{"location":"About/faqs/#ive-added-some-tools-do-you-want-my-changes","text":"Yes please. Do this via a pull request on github.","title":"I've added some tools, do you want my changes?"},{"location":"About/faqs/#there-is-an-errorhole-in-the-documentation","text":"You should just be able to edit and fix it on Github. Open the page and click the pencil icon, top right, to edit. Thanks for your help. If you think that parts of the documentation are lacking, and you would like to significantly extend them, we also welcome that. If you prefer to discuss it before you start then please do get in touch.","title":"There is an error/hole in the documentation"},{"location":"About/faqs/#it-works-on-one-machine-but-not-on-another","text":"Run-anywhere portability is a real challenge in bioinformatics, not just for Tapirs. Tapirs has been tested succesfully on Windows, OSX, and Linux, but problems may still arise. Conda environments have proved really useful for us, but they aren't perfect. As a start just check that both machines are running the most recent version of Conda and you have the most recent version of Tapirs. Make sure Conda is activated to the base environment, has snakemake installed (check with conda list ), and you are using the --use-conda flag in your initial snakemake command. You could also take the very detailed exported environment (reports/archived_envs) from the successful machine and use it to create a new environment on the not-running machine ( conda env create --file environment.yaml ). Activate this environment and do not use the --use-conda flag when running the initial snakemake command. Both machines should then have exactly the same software installed.","title":"It works on one machine but not on another"},{"location":"About/faqs/#who-do-i-contact-about-the-software","text":"The best way is to flag this on GitHub. If you need to get in touch personally then contact Dave Lunt on Gmail.","title":"Who do I contact about the software?"},{"location":"About/faqs/#is-it-licensed","text":"No, all parts that we have written are free under CC0 public domain and you may do as you wish. Yes, you may include whatever you wish into your own software, we encourage this. All software components that we have included (rather than written, see environment.yaml) have been chosen because they have permissive open source licences, but you may wish to check the details for yourself.","title":"Is it licensed?"},{"location":"About/faqs/#is-there-a-citation","text":"We greatly appreciate you citing us wherever you are able. A suitable citation might be: Title: Tapirs, an extensible workflow for reproducible metabarcoding Authors: URL: https://github.com/davelunt/Tapirs An appropriate way to do this would be to include references to the original analysis software: \"A reproducible metabarcoding workflow was implemented in Tapirs [1] using blast [2], Kraken2 [3], SINTAX [4] and Krona [5].\"","title":"Is there a citation"},{"location":"How-To-Guide/extending/","text":"Advice on extending the workflow by adding tools, and other approaches. Rationale What is the best approach for your data analysis? Whatever you think is the answer you will need to gather evidence. Workflow systems like snakemake allow relatively easy addition and extension of the analyses. This is very useful in comparative analyses. If a new paper claims taxonomic assignment method Y is better than method X then you may wish to carry out both and compare the results, while standardising the other parts of your workflow. Adding tools Although familiarity with Snakemake is helpful, in many situations you may be able to add a tool by modifying an existing rule. First make sure that your new method produces the results you expect when run at the command line. If it doesn't work at the command line it won't work in Snakemake. Snakemake rules have 3 integral parts; input, output, and the command to turn the first into the second. See the Snakemake Documentation Common problems Teaching and trouble-shooting Snakemake and bioinformatics are beyond the scope of this document. A couple of pointers however can save a lot of time. 1, Most problems are because you have a typo 2, If you have multiple lines of input or output each line except the last must finish in a comma. Look at the line that begins \"file1\" 3, Take note of the tabbed indentation structure as this can cause problems when incorrect. 4, The output of your rule must be added to the rule all in the snakefile. 5, Commands requiring \" or {} must be escaped through duplication. Eg, ls -l | awk '{if ($3 == \"rahmu\") print $0;}' becomes ls -l | awk '{{if ($3 == \"\"rahmu\"\") print $0;}}' 6, Here is an example rule. rule test: input: file1 = \"firstfile.fasta\", file2 = \"secondfile.fasta\" output: \"allseqs.fasta\" shell: \"cat {input.file1} {input.file2} >> {output}\" The graph can help you understand You should make a DAG to view the flow of information through your workflow. It is often possible to spot problems this way when you have added rules. It can also help in planning to add a rule, making you clearer on where the data comes from and where it goes. To create a DAG (requires graphviz installed) use: snakemake --rulegraph | dot -Tsvg > dag.svg To create a DAG also showing individual samples use: snakemake --dag | dot -Tsvg > dag.svg Contributing your improvements to Tapirs We would love to hear from you about the improvements you've made. A pull-request for your git branch would probably be best. Why not help improve the documentation? A new and slightly different tutorial is always welcome.","title":"Extending the Workflow"},{"location":"How-To-Guide/extending/#rationale","text":"What is the best approach for your data analysis? Whatever you think is the answer you will need to gather evidence. Workflow systems like snakemake allow relatively easy addition and extension of the analyses. This is very useful in comparative analyses. If a new paper claims taxonomic assignment method Y is better than method X then you may wish to carry out both and compare the results, while standardising the other parts of your workflow.","title":"Rationale"},{"location":"How-To-Guide/extending/#adding-tools","text":"Although familiarity with Snakemake is helpful, in many situations you may be able to add a tool by modifying an existing rule. First make sure that your new method produces the results you expect when run at the command line. If it doesn't work at the command line it won't work in Snakemake. Snakemake rules have 3 integral parts; input, output, and the command to turn the first into the second. See the Snakemake Documentation","title":"Adding tools"},{"location":"How-To-Guide/extending/#common-problems","text":"Teaching and trouble-shooting Snakemake and bioinformatics are beyond the scope of this document. A couple of pointers however can save a lot of time. 1, Most problems are because you have a typo 2, If you have multiple lines of input or output each line except the last must finish in a comma. Look at the line that begins \"file1\" 3, Take note of the tabbed indentation structure as this can cause problems when incorrect. 4, The output of your rule must be added to the rule all in the snakefile. 5, Commands requiring \" or {} must be escaped through duplication. Eg, ls -l | awk '{if ($3 == \"rahmu\") print $0;}' becomes ls -l | awk '{{if ($3 == \"\"rahmu\"\") print $0;}}' 6, Here is an example rule. rule test: input: file1 = \"firstfile.fasta\", file2 = \"secondfile.fasta\" output: \"allseqs.fasta\" shell: \"cat {input.file1} {input.file2} >> {output}\"","title":"Common problems"},{"location":"How-To-Guide/extending/#the-graph-can-help-you-understand","text":"You should make a DAG to view the flow of information through your workflow. It is often possible to spot problems this way when you have added rules. It can also help in planning to add a rule, making you clearer on where the data comes from and where it goes. To create a DAG (requires graphviz installed) use: snakemake --rulegraph | dot -Tsvg > dag.svg To create a DAG also showing individual samples use: snakemake --dag | dot -Tsvg > dag.svg","title":"The graph can help you understand"},{"location":"How-To-Guide/extending/#contributing-your-improvements-to-tapirs","text":"We would love to hear from you about the improvements you've made. A pull-request for your git branch would probably be best. Why not help improve the documentation? A new and slightly different tutorial is always welcome.","title":"Contributing your improvements to Tapirs"},{"location":"How-To-Guide/installation/","text":"Note These documents assume a unix system like OSX or Linux, but should be applicable to all systems Although you can install and run Tapirs without too many steps you will need some very basic knowledge of the command line. A basic knowledge of Snakemake will help you to modify and configure Tapirs. Snakemake is a relatively easy workflow manager, but we recommend that you familiarise yourself with it, perhaps carry out the tutorial . Install miniconda Conda (miniconda) is a package and environment manager and is required here to install software and their dependencies. Follow the installation instructions for Miniconda for your operating system. git clone tapirs Apple OSX and Linux should both come with git already installed. At the command line type git --version and you should see the version number. If instead it reports command not found: git or similar then it is not installed. You can go to the git site to get installation advice or slightly easier might be to try conda install git at the command line while within your conda base environment. If you have git installed then clone the repository to your local machine with git clone https://github.com/davelunt/Tapirs.git You could alternatively download the repository from the Tapirs github repository using the green \"Clone or download\" button. Then expand the zip file and navigate into the directory. Another way would to be to download the GitHub Desktop application and proceed from there. Install Snakemake in your base conda environment Once conda is installed you will see \"(base)\" at the start of your command prompt. If this is not the case run conda activate base . With (base) activated, install Snakemake: conda install -c bioconda -c conda-forge snakemake Snakemake is the only dependency that requires manually installing; it will install and manage all other packages itself using conda. Databases and data You should now have installed all the software required for your analysis. You will also require some data and databases however. In order to search a reference database with your query sequences you will need to provide both, and tell Tapirs where they are located. Databases are large files, and everyone needs a different one, so they are not included with Tapirs. Instructions of what files are required are provided on the Tapirs setup page.","title":"Installation"},{"location":"How-To-Guide/installation/#install-miniconda","text":"Conda (miniconda) is a package and environment manager and is required here to install software and their dependencies. Follow the installation instructions for Miniconda for your operating system.","title":"Install miniconda"},{"location":"How-To-Guide/installation/#git-clone-tapirs","text":"Apple OSX and Linux should both come with git already installed. At the command line type git --version and you should see the version number. If instead it reports command not found: git or similar then it is not installed. You can go to the git site to get installation advice or slightly easier might be to try conda install git at the command line while within your conda base environment. If you have git installed then clone the repository to your local machine with git clone https://github.com/davelunt/Tapirs.git You could alternatively download the repository from the Tapirs github repository using the green \"Clone or download\" button. Then expand the zip file and navigate into the directory. Another way would to be to download the GitHub Desktop application and proceed from there.","title":"git clone tapirs"},{"location":"How-To-Guide/installation/#install-snakemake-in-your-base-conda-environment","text":"Once conda is installed you will see \"(base)\" at the start of your command prompt. If this is not the case run conda activate base . With (base) activated, install Snakemake: conda install -c bioconda -c conda-forge snakemake Snakemake is the only dependency that requires manually installing; it will install and manage all other packages itself using conda.","title":"Install Snakemake in your base conda environment"},{"location":"How-To-Guide/installation/#databases-and-data","text":"You should now have installed all the software required for your analysis. You will also require some data and databases however. In order to search a reference database with your query sequences you will need to provide both, and tell Tapirs where they are located. Databases are large files, and everyone needs a different one, so they are not included with Tapirs. Instructions of what files are required are provided on the Tapirs setup page.","title":"Databases and data"},{"location":"How-To-Guide/reproducibility/","text":"Reproducible Analyses Reproducibility is important professionally, but the main person it is helping is future-you. Exit Strategy When an experiment is finished you should have an 'exit strategy' checklist to make sure your work is as reproducible as possible. We hope that we have made this achievable in Tapirs data provenance data archive is possible list of all software, sources, and versions conda export of environment, all software conda installable workflow human readable reports easy to archive Software list and versions The full list of software, their dependencies and version numbers called environment.yaml is written to envs/archived_envs directory at the end of the run. This file can be used to reproduce the experimental software conditions. Archiving Snakemake can be asked to make an archive of code, config, and input files: snakemake --archive my-workflow.tar.gz Transparent workflow","title":"Reproducibiility"},{"location":"How-To-Guide/reproducibility/#reproducible-analyses","text":"Reproducibility is important professionally, but the main person it is helping is future-you.","title":"Reproducible Analyses"},{"location":"How-To-Guide/reproducibility/#exit-strategy","text":"When an experiment is finished you should have an 'exit strategy' checklist to make sure your work is as reproducible as possible. We hope that we have made this achievable in Tapirs data provenance data archive is possible list of all software, sources, and versions conda export of environment, all software conda installable workflow human readable reports easy to archive","title":"Exit Strategy"},{"location":"How-To-Guide/reproducibility/#software-list-and-versions","text":"The full list of software, their dependencies and version numbers called environment.yaml is written to envs/archived_envs directory at the end of the run. This file can be used to reproduce the experimental software conditions.","title":"Software list and versions"},{"location":"How-To-Guide/reproducibility/#archiving","text":"Snakemake can be asked to make an archive of code, config, and input files: snakemake --archive my-workflow.tar.gz","title":"Archiving"},{"location":"How-To-Guide/reproducibility/#transparent-workflow","text":"","title":"Transparent workflow"},{"location":"How-To-Guide/setup/","text":"install first then setup These instructions are to set up Tapirs for your experiment after installation has been carried out. CONFIG FILES The config.yaml file contains settings for the workflows operation. In an ideal experiment you would never have to alter any of the snakemake workflow code, only set up a configuration text file. Most of the settings in config.yaml have reasonable default values, but some may require your input. Open config.yaml in a text editor. set a name for your experiment (default=expt_name) set the directory containing your demultiplexed fastq.gz data (default=data/01_demultiplexed) check the locations of your reference sequence databases are correct and amend if they are elsewhere. Defaults are: data/databases/blast data/databases/kraken2 data/databases/taxonomy data/databases/sintax In addition you may wish to fine-tune the analysis parameters of the programs. These parameters have reasonable defaults and changing them is not compulsory (unlike the options above, where you must identify your data). set fastp parameters for quality control and read merging set blast parameters set MLCA parameters DATA The data/ directory must contain 2 subdirectories 01_demultiplexed/ databases/ 01_demultiplexed The start point of this workflows is demultiplexed fastq.gz sequence files in the data/01_demultiplexed directory. There are many ways to produce these files, and your sequencing machine or sequencing centre will most likely return this as the default data. It is essential for reproducibility that this starting data is kept together with the experiment. If size prohibits its inclusion then archive it publicly and include the doi into the experimental record. tip: exclude data directory from version control Remember that if you are wisely keeping your analysis under git version control, then you can exclude very large data files from syncing to the web by including the data directory in your .gitignore file. It is essential however that you make other arrangements for both backing up and publishing the data. To facilitate iteration, the input files must follow the structure of yourlibrary/yoursample.R*.fastq.gz , the full path being Tapirs/data/01_demultiplexed/yourlibrary/yoursample.R*.fastq.gz . libraries and samples lists Rather than implement a complex set of wildcards the recommended approach for snakemake is to create textual lists (.tsv files) of the sequencing metadata [Note1]. These lists can of course be generated by a script, but importantly this allows quality control and human intervention in determining what samples are analysed in each run. We then use the powerful Pandas data analysis library to read information from each and guide the workflow over the data. Note 1: samples, libraries, and units .tsv The naming of the files differs between biological disciplines. In our work a library often represents a physical location (\"Lake Windermere\") and a sample represents a physical unit taken for DNA extraction (\"water-sample-12\"). In other disciplines the meaning of \"sample\" may differ slightly. Tapirs contains a custom bash script to create two text files libraries.tsv and samples.tsv each containing a list of all library and sample names respectively, performed with: bash scripts/wildcarding.sh You should make sure that the config.yaml correctly points at these files. These will need removing and regenerating if libraries or samples changeinput Databases Kraken2 In order to run a Kraken analysis you will need to create a Kraken database for the program to search your query sequences against. These databases can be large and require a lot of RAM and time to build them. They only need to be made once however and then all subsequent analyses can be performed against the same database. If you are building a custom database from a few thousand sequences then database construction will likely be quick. Creating a local Kraken2 database for Tapirs requires a fasta input file containing all reference sequences. Database creation has 3 steps: 1. Download taxonomy: kraken2-build --download-taxonomy --db fish_kraken --threads 6 2. Add sequences to library: kraken2-build --add-to-library databases/12s_verts.fasta --no-masking --db fish_kraken --threads 6 the --no-masking flag improved accuracy for short reads 3. Build database: kraken2-build --build --minimizer-spaces 1 --kmer-len 31 --db fish_kraken --threads 6 it is important to have some minimizer-spaces in lmers, having 1 makes for more accurate taxonomic assignment. Having kmer same length as lmer for shorter reads makes for more accurate taxonomic assignment See the Kraken Tutorial and the Kraken2 manual for information on building Kraken databases. It is essential for reproducibility that you publicly archive your database at the end of the experiment. Zenodo.org is a suitable location. blast You will need to build a blastn database from a collection of fasta files. Information on this can be found at the NCBI site Blast help pages . You will require: * Fasta input file containing all reference sequences * Accession to taxid map. See NCBI blast instructions for more details. create a blast database If you have a single fasta format file (allseqs.fas) containing all the sequences to be included in the database, then you could create a blast database with the command: makeblastdb -in blast_db/allseqs.fasta -input_type fasta -dbtype nucl \\ -parse_seqids -taxid_map blast_db/tax_map.txt -out blast_db/allseqs See the Blast Tutorial for more detailed help It is essential for reproducibility that you publicly archive your database at the end of the experiment. Zenodo.org is a suitable location. taxonomy database Several programs require the NCBI taxonomy database in order to carry out taxonomic assignment. SINTAX SINTAX (Edgar 2016) is a kmer similarity approach to taxonomic ID that classifies and gives confidence estimates on the classification. It requires a database (fasta, fastq, or udb database format) in the data/databsaes/sintax directory. A sintax database is created using specific taxonomic information in the header line of the fasta file. We provide a script to help with this formatting. krona Krona should self-install it's taxonomy database from the snakemake rule supplied. DRY RUN TAPIRS Make sure you are in the directory containing the snakefile then type snakefile --use-conda -npr or snakemake -s snakefile --use-conda --printshellcmds -n -k to dry-run the workflow. If all has gone well Snakemake will report the jobs it needs to perform without any complaint. If not (as is common in most experiments) you will need to diagnose and fix any minor issues. Some errors are only detected in the real run, not the dry run, and they often concern the format of data files, as these have not been checked by a dry run. RUN TAPIRS Run Tapirs with either the snakemake --use-conda or snakemake -s snakefile --use-conda --printshellcmds command Tapirs should now run, processing the data from 01_demultiplexed, assigning taxonomy using blast, kraken2 and sintax, writing reports, and creating html displays of the taxonomic composition of each sample using krona. REFERENCES Altschul, S. F. et al. (1990) \u2018Basic local alignment search tool\u2019, Journal of molecular biology, 215(3), pp. 403\u2013410. doi: 10.1016/S0022-2836(05)80360-2 Chen, S. et al. (2018) \u2018fastp: an ultra-fast all-in-one FASTQ preprocessor\u2019, Bioinformatics . Oxford University Press, 34(17), pp. i884\u2013i890. doi: 10.1093/bioinformatics/bty560 Daniel McDonald, Jose C. Clemente, Justin Kuczynski, Jai Ram Rideout, Jesse Stombaugh, Doug Wendel, Andreas Wilke, Susan Huse, John Hufnagle, Folker Meyer, Rob Knight, and J. Gregory Caporaso. The Biological Observation Matrix (BIOM) format or: how I learned to stop worrying and love the ome-ome. GigaScience 2012, 1:7. doi:10.1186/2047-217X-1-7 Ondov, B. D., Bergman, N. H. and Phillippy, A. M. (2011) \u2018Interactive metagenomic visualization in a Web browser\u2019, BMC bioinformatics, 12, p. 385. doi: 10.1186/1471-2105-12-385 Rognes, T. et al. (2016) \u2018VSEARCH: a versatile open source tool for metagenomics\u2019, PeerJ, 4, p. e2584. [doi: 10.7717/peerj.2584] Wood, D. E., Lu, J. and Langmead, B. (2019) \u2018Improved metagenomic analysis with Kraken 2\u2019, Genome biology, 20(1), p. 257. doi: 10.1186/s13059-019-1891-0 R.C. Edgar (2016), SINTAX: a simple non-Bayesian taxonomy classifier for 16S and ITS sequences, https://doi.org/10.1101/074161","title":"Setup Tapirs"},{"location":"How-To-Guide/setup/#config-files","text":"The config.yaml file contains settings for the workflows operation. In an ideal experiment you would never have to alter any of the snakemake workflow code, only set up a configuration text file. Most of the settings in config.yaml have reasonable default values, but some may require your input. Open config.yaml in a text editor. set a name for your experiment (default=expt_name) set the directory containing your demultiplexed fastq.gz data (default=data/01_demultiplexed) check the locations of your reference sequence databases are correct and amend if they are elsewhere. Defaults are: data/databases/blast data/databases/kraken2 data/databases/taxonomy data/databases/sintax In addition you may wish to fine-tune the analysis parameters of the programs. These parameters have reasonable defaults and changing them is not compulsory (unlike the options above, where you must identify your data). set fastp parameters for quality control and read merging set blast parameters set MLCA parameters","title":"CONFIG FILES"},{"location":"How-To-Guide/setup/#data","text":"The data/ directory must contain 2 subdirectories 01_demultiplexed/ databases/","title":"DATA"},{"location":"How-To-Guide/setup/#01_demultiplexed","text":"The start point of this workflows is demultiplexed fastq.gz sequence files in the data/01_demultiplexed directory. There are many ways to produce these files, and your sequencing machine or sequencing centre will most likely return this as the default data. It is essential for reproducibility that this starting data is kept together with the experiment. If size prohibits its inclusion then archive it publicly and include the doi into the experimental record. tip: exclude data directory from version control Remember that if you are wisely keeping your analysis under git version control, then you can exclude very large data files from syncing to the web by including the data directory in your .gitignore file. It is essential however that you make other arrangements for both backing up and publishing the data. To facilitate iteration, the input files must follow the structure of yourlibrary/yoursample.R*.fastq.gz , the full path being Tapirs/data/01_demultiplexed/yourlibrary/yoursample.R*.fastq.gz .","title":"01_demultiplexed"},{"location":"How-To-Guide/setup/#libraries-and-samples-lists","text":"Rather than implement a complex set of wildcards the recommended approach for snakemake is to create textual lists (.tsv files) of the sequencing metadata [Note1]. These lists can of course be generated by a script, but importantly this allows quality control and human intervention in determining what samples are analysed in each run. We then use the powerful Pandas data analysis library to read information from each and guide the workflow over the data. Note 1: samples, libraries, and units .tsv The naming of the files differs between biological disciplines. In our work a library often represents a physical location (\"Lake Windermere\") and a sample represents a physical unit taken for DNA extraction (\"water-sample-12\"). In other disciplines the meaning of \"sample\" may differ slightly. Tapirs contains a custom bash script to create two text files libraries.tsv and samples.tsv each containing a list of all library and sample names respectively, performed with: bash scripts/wildcarding.sh You should make sure that the config.yaml correctly points at these files. These will need removing and regenerating if libraries or samples changeinput","title":"libraries and samples lists"},{"location":"How-To-Guide/setup/#databases","text":"","title":"Databases"},{"location":"How-To-Guide/setup/#kraken2","text":"In order to run a Kraken analysis you will need to create a Kraken database for the program to search your query sequences against. These databases can be large and require a lot of RAM and time to build them. They only need to be made once however and then all subsequent analyses can be performed against the same database. If you are building a custom database from a few thousand sequences then database construction will likely be quick. Creating a local Kraken2 database for Tapirs requires a fasta input file containing all reference sequences. Database creation has 3 steps: 1. Download taxonomy: kraken2-build --download-taxonomy --db fish_kraken --threads 6 2. Add sequences to library: kraken2-build --add-to-library databases/12s_verts.fasta --no-masking --db fish_kraken --threads 6 the --no-masking flag improved accuracy for short reads 3. Build database: kraken2-build --build --minimizer-spaces 1 --kmer-len 31 --db fish_kraken --threads 6 it is important to have some minimizer-spaces in lmers, having 1 makes for more accurate taxonomic assignment. Having kmer same length as lmer for shorter reads makes for more accurate taxonomic assignment See the Kraken Tutorial and the Kraken2 manual for information on building Kraken databases. It is essential for reproducibility that you publicly archive your database at the end of the experiment. Zenodo.org is a suitable location.","title":"Kraken2"},{"location":"How-To-Guide/setup/#blast","text":"You will need to build a blastn database from a collection of fasta files. Information on this can be found at the NCBI site Blast help pages . You will require: * Fasta input file containing all reference sequences * Accession to taxid map. See NCBI blast instructions for more details. create a blast database If you have a single fasta format file (allseqs.fas) containing all the sequences to be included in the database, then you could create a blast database with the command: makeblastdb -in blast_db/allseqs.fasta -input_type fasta -dbtype nucl \\ -parse_seqids -taxid_map blast_db/tax_map.txt -out blast_db/allseqs See the Blast Tutorial for more detailed help It is essential for reproducibility that you publicly archive your database at the end of the experiment. Zenodo.org is a suitable location.","title":"blast"},{"location":"How-To-Guide/setup/#taxonomy-database","text":"Several programs require the NCBI taxonomy database in order to carry out taxonomic assignment.","title":"taxonomy database"},{"location":"How-To-Guide/setup/#sintax","text":"SINTAX (Edgar 2016) is a kmer similarity approach to taxonomic ID that classifies and gives confidence estimates on the classification. It requires a database (fasta, fastq, or udb database format) in the data/databsaes/sintax directory. A sintax database is created using specific taxonomic information in the header line of the fasta file. We provide a script to help with this formatting.","title":"SINTAX"},{"location":"How-To-Guide/setup/#krona","text":"Krona should self-install it's taxonomy database from the snakemake rule supplied.","title":"krona"},{"location":"How-To-Guide/setup/#dry-run-tapirs","text":"Make sure you are in the directory containing the snakefile then type snakefile --use-conda -npr or snakemake -s snakefile --use-conda --printshellcmds -n -k to dry-run the workflow. If all has gone well Snakemake will report the jobs it needs to perform without any complaint. If not (as is common in most experiments) you will need to diagnose and fix any minor issues. Some errors are only detected in the real run, not the dry run, and they often concern the format of data files, as these have not been checked by a dry run.","title":"DRY RUN TAPIRS"},{"location":"How-To-Guide/setup/#run-tapirs","text":"Run Tapirs with either the snakemake --use-conda or snakemake -s snakefile --use-conda --printshellcmds command Tapirs should now run, processing the data from 01_demultiplexed, assigning taxonomy using blast, kraken2 and sintax, writing reports, and creating html displays of the taxonomic composition of each sample using krona. REFERENCES Altschul, S. F. et al. (1990) \u2018Basic local alignment search tool\u2019, Journal of molecular biology, 215(3), pp. 403\u2013410. doi: 10.1016/S0022-2836(05)80360-2 Chen, S. et al. (2018) \u2018fastp: an ultra-fast all-in-one FASTQ preprocessor\u2019, Bioinformatics . Oxford University Press, 34(17), pp. i884\u2013i890. doi: 10.1093/bioinformatics/bty560 Daniel McDonald, Jose C. Clemente, Justin Kuczynski, Jai Ram Rideout, Jesse Stombaugh, Doug Wendel, Andreas Wilke, Susan Huse, John Hufnagle, Folker Meyer, Rob Knight, and J. Gregory Caporaso. The Biological Observation Matrix (BIOM) format or: how I learned to stop worrying and love the ome-ome. GigaScience 2012, 1:7. doi:10.1186/2047-217X-1-7 Ondov, B. D., Bergman, N. H. and Phillippy, A. M. (2011) \u2018Interactive metagenomic visualization in a Web browser\u2019, BMC bioinformatics, 12, p. 385. doi: 10.1186/1471-2105-12-385 Rognes, T. et al. (2016) \u2018VSEARCH: a versatile open source tool for metagenomics\u2019, PeerJ, 4, p. e2584. [doi: 10.7717/peerj.2584] Wood, D. E., Lu, J. and Langmead, B. (2019) \u2018Improved metagenomic analysis with Kraken 2\u2019, Genome biology, 20(1), p. 257. doi: 10.1186/s13059-019-1891-0 R.C. Edgar (2016), SINTAX: a simple non-Bayesian taxonomy classifier for 16S and ITS sequences, https://doi.org/10.1101/074161","title":"RUN TAPIRS"},{"location":"Tutorials/blast_tutorial/","text":"Basic blast tutorial Reference data Download the sequences you wish to include in your reference database. Build a blast database There are many online guides to building a blast database, including detailed ones at the NCBI. blast parameters Minimum percentage identity: min_perc_ident = \"100\" identifies perfect matches but may be a little restrictive unless you know that you have all the potential hits in you database. Lower the value on a few test cases until you feel you are getting too much noise and not enough signal. Somewhere between that point and \"100\" is your optimum. e-value: min_evalue = \"1e-20\" may work for you, again you can explore hw stringent to be with this parameter. Using the NCBI web blast page for a few sequences can sometimes give you an idea of the sort of e-values you would expect for your data. number of hits: descriptions = \"50\" ie return maximum of 50 hits is a good default, though you may also change if you wish. mlca Investigating the correct parameters for LCA analysis is an overlooked part of many experimental designs. LCA however is responsible for turning your 50 blast hits into an assigned taxonomy, so it can be exceptionally important. MLCA is short for Majority Lowest Common Ancestor, and our script allows you to modify several parameters of the LCA estimation, including whether to accept the majority taxonomy returned, or require ALL hits to have the same taxonomy. bitscore threshold: This will restrict the hits to a certain percentage of the top bitscore. Default=10 (within 10% of top hit) which may be a little too strict if you do not have close blast hits. Experiment with reducing it. identity: Minimum percentage identity value accepted for the hits. \"100\" restricts to perfect matches only. coverage: Percentage coverage of query on top hit. Default = \"60\" majority taxonomic concensus: Percentage match requirement across hit taxonomies. \"90\" indicates that 1 out of 10 may disagree in its taxonomy without the assigned taxonomy moving back to a deeper common ancestor. minimum number of hits: Useful with small reference databases. Default = 2, ie requires at least 2 hits in the blast database. hits = \"1\" is not a true LCA but rather takes the top hit.","title":"Basic blast"},{"location":"Tutorials/blast_tutorial/#basic-blast-tutorial","text":"","title":"Basic blast tutorial"},{"location":"Tutorials/blast_tutorial/#reference-data","text":"Download the sequences you wish to include in your reference database.","title":"Reference data"},{"location":"Tutorials/blast_tutorial/#build-a-blast-database","text":"There are many online guides to building a blast database, including detailed ones at the NCBI.","title":"Build a blast database"},{"location":"Tutorials/blast_tutorial/#blast-parameters","text":"Minimum percentage identity: min_perc_ident = \"100\" identifies perfect matches but may be a little restrictive unless you know that you have all the potential hits in you database. Lower the value on a few test cases until you feel you are getting too much noise and not enough signal. Somewhere between that point and \"100\" is your optimum. e-value: min_evalue = \"1e-20\" may work for you, again you can explore hw stringent to be with this parameter. Using the NCBI web blast page for a few sequences can sometimes give you an idea of the sort of e-values you would expect for your data. number of hits: descriptions = \"50\" ie return maximum of 50 hits is a good default, though you may also change if you wish.","title":"blast parameters"},{"location":"Tutorials/blast_tutorial/#mlca","text":"Investigating the correct parameters for LCA analysis is an overlooked part of many experimental designs. LCA however is responsible for turning your 50 blast hits into an assigned taxonomy, so it can be exceptionally important. MLCA is short for Majority Lowest Common Ancestor, and our script allows you to modify several parameters of the LCA estimation, including whether to accept the majority taxonomy returned, or require ALL hits to have the same taxonomy. bitscore threshold: This will restrict the hits to a certain percentage of the top bitscore. Default=10 (within 10% of top hit) which may be a little too strict if you do not have close blast hits. Experiment with reducing it. identity: Minimum percentage identity value accepted for the hits. \"100\" restricts to perfect matches only. coverage: Percentage coverage of query on top hit. Default = \"60\" majority taxonomic concensus: Percentage match requirement across hit taxonomies. \"90\" indicates that 1 out of 10 may disagree in its taxonomy without the assigned taxonomy moving back to a deeper common ancestor. minimum number of hits: Useful with small reference databases. Default = 2, ie requires at least 2 hits in the blast database. hits = \"1\" is not a true LCA but rather takes the top hit.","title":"mlca"},{"location":"Tutorials/kraken_tutorial/","text":"Kraken databases Kraken output formats","title":"Kraken2"},{"location":"Tutorials/kraken_tutorial/#kraken-databases","text":"","title":"Kraken databases"},{"location":"Tutorials/kraken_tutorial/#kraken-output-formats","text":"","title":"Kraken output formats"},{"location":"Tutorials/sintax_tutorial/","text":"SINTAX tutorial Reference data Download the sequences you wish to include in your reference database. Build a SINTAX database parameters convert output for krona","title":"SINTAX"},{"location":"Tutorials/sintax_tutorial/#sintax-tutorial","text":"","title":"SINTAX tutorial"},{"location":"Tutorials/sintax_tutorial/#reference-data","text":"Download the sequences you wish to include in your reference database.","title":"Reference data"},{"location":"Tutorials/sintax_tutorial/#build-a-sintax-database","text":"","title":"Build a SINTAX database"},{"location":"Tutorials/sintax_tutorial/#parameters","text":"","title":"parameters"},{"location":"Tutorials/sintax_tutorial/#convert-output-for-krona","text":"","title":"convert output for krona"}]}