{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview of Tapirs Tapirs is a Snakemake workflow system to reproducibly process metabarcode DNA sequences and assign taxonomy. The Tapirs workflow starts with a directory of demultiplexed fastq.gz sequences. There are three key sections to its workflow: Quality Control quality trimming, length trimming, denoising to remove errors, and dereplication to remove redundancy Taxonomic Assignment Taxonomic identity is assigned to each sequence by a variety of methods including blast with different LCA approaches, and Karken2. Since we use a workflow manager (Snakemake) methods can easily be added to this list without affecting the rest of the workflow Reports and Graphical Display The workflow will write a detailed report of its analyses and actions, and output to standard format BIOM and .tsv files. Krona is used to create interactive html graphical displays of the data. The Vegan R package is used to calculate diversity statistics and plots. Quickstart install conda (miniconda) git clone the Tapirs repository git clone https://github.com/davelunt/Tapirs create and activate a conda environment from the Tapirs environment.yaml file conda env create --file environment.yaml edit the config.yaml to identify the location of demultiplexed data and databases dry run snakemake -npr to identify any issues run snakemake See the installation and setup pages for more detailed help Licence and citation Project led by Dave Lunt , Mike Winter, Graham Sellers, Marco Benucci, and the EvoHull group at the University of Hull, UK. The software is released as CC0, public domain, you may do as you wish. Please cite the software like this: Title: Tapirs: extensible reproducible workflows for metabarcoding Authors: doi: 1234567 URL: https://github.com/davelunt/Tapirs Please also cite the software generating the analyses. An appropriate way to do this would be: \"A reproducible metabarcoding workflow was implemented in Tapirs [1] using vsearch [2], blast [3], Kraken2 [4], and Krona [5].\"","title":"Home"},{"location":"#overview-of-tapirs","text":"Tapirs is a Snakemake workflow system to reproducibly process metabarcode DNA sequences and assign taxonomy. The Tapirs workflow starts with a directory of demultiplexed fastq.gz sequences. There are three key sections to its workflow: Quality Control quality trimming, length trimming, denoising to remove errors, and dereplication to remove redundancy Taxonomic Assignment Taxonomic identity is assigned to each sequence by a variety of methods including blast with different LCA approaches, and Karken2. Since we use a workflow manager (Snakemake) methods can easily be added to this list without affecting the rest of the workflow Reports and Graphical Display The workflow will write a detailed report of its analyses and actions, and output to standard format BIOM and .tsv files. Krona is used to create interactive html graphical displays of the data. The Vegan R package is used to calculate diversity statistics and plots.","title":"Overview of Tapirs"},{"location":"#quickstart","text":"install conda (miniconda) git clone the Tapirs repository git clone https://github.com/davelunt/Tapirs create and activate a conda environment from the Tapirs environment.yaml file conda env create --file environment.yaml edit the config.yaml to identify the location of demultiplexed data and databases dry run snakemake -npr to identify any issues run snakemake See the installation and setup pages for more detailed help","title":"Quickstart"},{"location":"#licence-and-citation","text":"Project led by Dave Lunt , Mike Winter, Graham Sellers, Marco Benucci, and the EvoHull group at the University of Hull, UK. The software is released as CC0, public domain, you may do as you wish. Please cite the software like this: Title: Tapirs: extensible reproducible workflows for metabarcoding Authors: doi: 1234567 URL: https://github.com/davelunt/Tapirs Please also cite the software generating the analyses. An appropriate way to do this would be: \"A reproducible metabarcoding workflow was implemented in Tapirs [1] using vsearch [2], blast [3], Kraken2 [4], and Krona [5].\"","title":"Licence and citation"},{"location":"About/background/","text":"Background We developed Tapirs primarily for our own work in the EvoHull group at the University of Hull, UK. It has been common for us to carry out short (Illumina) metabarcoding on environmental or eDNA community samples. Motivation We wanted a transparent workflow that was not dependent on the person who developed the software, could be understood easily by new people in the lab, and enabled us to take reproducibility seriously. We wanted to be able to vary conditions and reanalyse data experimentally, and that meant that changing parameters and re-running should be easy. We had previously used our own metabarcoding software metaBEAT , written as a large python script, and while very productive we found that supporting it was increasingly difficult, and adding new analyses to this pipeline is not straightforward. We learned a great deal from designing and using metaBEAT, and it greatly increased our interest in workflow management software. Other software In addition to metaBEAT there are many other wonderful software packages for sophisticated analysis of metabarcoding data. QIIME2 is particularly powerful, as is Mothur, and too many others to mention. We felt that other metabarcoding software did not allow us to easily change the analysis software, but rather presented a fixed 'solution'. We saw a lot value for our research in having a standard workflow manager, with design simplicity, rather than the bigger more powerful but more opaque and fixed packages designed primarily for 16S bacterial sequencing. Using the Snakemake workflow management system We decided that it would be best practice to use a well-designed workflow manager rather than link together our software components in a more ad hoc manner with our own scripts. We chose the Snakemake workflow management system . This is well-designed software, heavily used and tested across a large community of bioinformatics researchers. We explicitly decided to use a specialist workflow manager, rather than designing a pipeline system ourselves, as we valued the ability to modify and experiment with our workflow, and this is by definition what workflow managers do. Reproducibility We take reproducible science very seriously, and so should you. Not only is it the highest quality science, but it is also the easiest science, saving you time and effort. We hope that the way we have designed Tapirs will give others a very good chance to exactly repeat what you did without significant suffering, and also to use their own data with your exact approach to build upon your work. Please see the Reproducibility page for details of maximising the reproducibility of your experiment. We would greatly appreciate your thoughts on better reproducibility in metabarcoding. Contributions Very many people have contributed to Tapirs. EvoHull: Many scientists have worked in EvoHull, discussed metabarcoding software and approaches, made suggestions about good practice, explored data analysis, critiqued this work, made suggestions, and taught us how to proceed. The code here was physically written by Dave Lunt, Mike Winter, Graham Sellers, and Marco Benucci.","title":"Background"},{"location":"About/background/#background","text":"We developed Tapirs primarily for our own work in the EvoHull group at the University of Hull, UK. It has been common for us to carry out short (Illumina) metabarcoding on environmental or eDNA community samples.","title":"Background"},{"location":"About/background/#motivation","text":"We wanted a transparent workflow that was not dependent on the person who developed the software, could be understood easily by new people in the lab, and enabled us to take reproducibility seriously. We wanted to be able to vary conditions and reanalyse data experimentally, and that meant that changing parameters and re-running should be easy. We had previously used our own metabarcoding software metaBEAT , written as a large python script, and while very productive we found that supporting it was increasingly difficult, and adding new analyses to this pipeline is not straightforward. We learned a great deal from designing and using metaBEAT, and it greatly increased our interest in workflow management software.","title":"Motivation"},{"location":"About/background/#other-software","text":"In addition to metaBEAT there are many other wonderful software packages for sophisticated analysis of metabarcoding data. QIIME2 is particularly powerful, as is Mothur, and too many others to mention. We felt that other metabarcoding software did not allow us to easily change the analysis software, but rather presented a fixed 'solution'. We saw a lot value for our research in having a standard workflow manager, with design simplicity, rather than the bigger more powerful but more opaque and fixed packages designed primarily for 16S bacterial sequencing.","title":"Other software"},{"location":"About/background/#using-the-snakemake-workflow-management-system","text":"We decided that it would be best practice to use a well-designed workflow manager rather than link together our software components in a more ad hoc manner with our own scripts. We chose the Snakemake workflow management system . This is well-designed software, heavily used and tested across a large community of bioinformatics researchers. We explicitly decided to use a specialist workflow manager, rather than designing a pipeline system ourselves, as we valued the ability to modify and experiment with our workflow, and this is by definition what workflow managers do.","title":"Using the Snakemake workflow management system"},{"location":"About/background/#reproducibility","text":"We take reproducible science very seriously, and so should you. Not only is it the highest quality science, but it is also the easiest science, saving you time and effort. We hope that the way we have designed Tapirs will give others a very good chance to exactly repeat what you did without significant suffering, and also to use their own data with your exact approach to build upon your work. Please see the Reproducibility page for details of maximising the reproducibility of your experiment. We would greatly appreciate your thoughts on better reproducibility in metabarcoding.","title":"Reproducibility"},{"location":"About/background/#contributions","text":"Very many people have contributed to Tapirs. EvoHull: Many scientists have worked in EvoHull, discussed metabarcoding software and approaches, made suggestions about good practice, explored data analysis, critiqued this work, made suggestions, and taught us how to proceed. The code here was physically written by Dave Lunt, Mike Winter, Graham Sellers, and Marco Benucci.","title":"Contributions"},{"location":"About/design/","text":"DIRECTORY STRUCTURE We set directory structure in the repository to standardise the workflow design. Above: the directory structure of a standard Tapirs workflow snakefile The top level snakefile to control the workflow. config file config.yaml is the experiment specific control file. You should edit this to name your experiment, specify the location of data, and edit any variable. envs conda environments for each rule can be specified here. These are in addition to the general top-level environment.yaml and may not be required. Data raw data Never analyse your raw data, make a copy to this folder. It then forms part of your experimental record. demultiplexed data We assume your raw data has already been demultiplexed and exists as .fastq.gz format files in a directory specified in the config.yaml file. Since de-multiplexing is different in different laboratories we teat this as a separate workflow. rules This contains snakemake workflow description rules for separate tasks. We may have: - blast.smk - kraken.smk - qc.smk - report.smk Each of these snakemake rules is run by the main snakefile at the appropriate time. Having separate rules for different sections of the workflow (eg quality control, qc.smk) allows better organisation and simplification of each component within the workflow. In our experience this makes the workflow much more understandable and easier to modify. reports Reports are written by some of the programs. Snakemake will also write a report. scripts Place here scripts called by the snakemake rules results This directory usually has subdirectories named by the program (eg blast). It is a convenient way of organising the output. docs Documentation for Tapirs ANALYSIS fastp fastp is used for 2 separate jobs: quality control, filter reads by score and length merge Forward and reverse reads. It additionally does error correction if merged reads have a mismatch, taking the highest quality nucleotide vsearch vsearch does several jobs: conversion of fastq.gz to fasta file format dereplication, removal of identical sequences denoising, removal of sequencing errors chimera removal The sequences written by vsearch are the query sequences for taxonomic identification. blast and BASTA-LCA blast blastn is used to search a pre-prepared database for sequence matches. The hits for each query sequence are written including a taxonomic identifier. BASTA BASTA will determine the Last Common Ancestor (LCA) of the blast hits for each query sequence. Kraken2 Kraken2 is an alternative to blast and BASTA-LCA. It uses a k-mer approach to determine taxonomy of query sequences by comparing to a databases built from reference sequences and taxonomic information. Databases are large, and require significant RAM and time to produce and their construction is not part of this workflow. Kraken2 searches themselves however are very fast and efficient. graphical display of results Krona is used to make an interactive html page to display taxonomic summaries","title":"Design"},{"location":"About/design/#directory-structure","text":"We set directory structure in the repository to standardise the workflow design. Above: the directory structure of a standard Tapirs workflow","title":"DIRECTORY STRUCTURE"},{"location":"About/design/#snakefile","text":"The top level snakefile to control the workflow.","title":"snakefile"},{"location":"About/design/#config-file","text":"config.yaml is the experiment specific control file. You should edit this to name your experiment, specify the location of data, and edit any variable.","title":"config file"},{"location":"About/design/#envs","text":"conda environments for each rule can be specified here. These are in addition to the general top-level environment.yaml and may not be required.","title":"envs"},{"location":"About/design/#data","text":"","title":"Data"},{"location":"About/design/#raw-data","text":"Never analyse your raw data, make a copy to this folder. It then forms part of your experimental record.","title":"raw data"},{"location":"About/design/#demultiplexed-data","text":"We assume your raw data has already been demultiplexed and exists as .fastq.gz format files in a directory specified in the config.yaml file. Since de-multiplexing is different in different laboratories we teat this as a separate workflow.","title":"demultiplexed data"},{"location":"About/design/#rules","text":"This contains snakemake workflow description rules for separate tasks. We may have: - blast.smk - kraken.smk - qc.smk - report.smk Each of these snakemake rules is run by the main snakefile at the appropriate time. Having separate rules for different sections of the workflow (eg quality control, qc.smk) allows better organisation and simplification of each component within the workflow. In our experience this makes the workflow much more understandable and easier to modify.","title":"rules"},{"location":"About/design/#reports","text":"Reports are written by some of the programs. Snakemake will also write a report.","title":"reports"},{"location":"About/design/#scripts","text":"Place here scripts called by the snakemake rules","title":"scripts"},{"location":"About/design/#results","text":"This directory usually has subdirectories named by the program (eg blast). It is a convenient way of organising the output.","title":"results"},{"location":"About/design/#docs","text":"Documentation for Tapirs","title":"docs"},{"location":"About/design/#analysis","text":"","title":"ANALYSIS"},{"location":"About/design/#fastp","text":"fastp is used for 2 separate jobs: quality control, filter reads by score and length merge Forward and reverse reads. It additionally does error correction if merged reads have a mismatch, taking the highest quality nucleotide","title":"fastp"},{"location":"About/design/#vsearch","text":"vsearch does several jobs: conversion of fastq.gz to fasta file format dereplication, removal of identical sequences denoising, removal of sequencing errors chimera removal The sequences written by vsearch are the query sequences for taxonomic identification.","title":"vsearch"},{"location":"About/design/#blast-and-basta-lca","text":"","title":"blast and BASTA-LCA"},{"location":"About/design/#blast","text":"blastn is used to search a pre-prepared database for sequence matches. The hits for each query sequence are written including a taxonomic identifier.","title":"blast"},{"location":"About/design/#basta","text":"BASTA will determine the Last Common Ancestor (LCA) of the blast hits for each query sequence.","title":"BASTA"},{"location":"About/design/#kraken2","text":"Kraken2 is an alternative to blast and BASTA-LCA. It uses a k-mer approach to determine taxonomy of query sequences by comparing to a databases built from reference sequences and taxonomic information. Databases are large, and require significant RAM and time to produce and their construction is not part of this workflow. Kraken2 searches themselves however are very fast and efficient.","title":"Kraken2"},{"location":"About/design/#graphical-display-of-results","text":"Krona is used to make an interactive html page to display taxonomic summaries","title":"graphical display of results"},{"location":"About/faqs/","text":"Frequently Asked Questions Why is it called Tapirs? What does it stand for? Tapirs is not an acronym, we just called it Tapirs because an easy name helps everyone, and tapirs are cool. Why did you make this and not use X Nothing met our needs, see the background page for discussion. We wanted an extensible workflow, built by a specialist workflow manager, that would allow real reproducibility. Can you add in my favourite software for me? No, sorry. We really hope however that choosing a workflow manager (Snakemake) means that this is much easier than for any other metabarcoding package. Look at the extending Tapirs How-To guide . I've added some tools, do you want my changes? Yes please. Do this via a pull request on github. There is an error in the documentation You should just be able to edit and fix it on Github. Open the page and click the pencil icon, top right, to edit. Thanks for your help. Who do I contact about the software? The best way is to flag this on GitHub. If you need to get in touch personally then contact Dave Lunt on Gmail. Is it licensed? No, all parts that we have written are free under CC0 public domain and you may do as you wish. Yes, you may include whatever you wish into your own software, we encourage this. All software components that we have included (rather than written, see environment.yaml) have been chosen because they have permissive open source licences, but you may wish to check the details for yourself. Is there a citation We greatly appreciate you citing us wherever you are able. A suitable citation might be: Title: Tapirs, an extensible workflow for reproducible metabarcoding Authors: URL: https://github.com/davelunt/Tapirs An appropriate way to do this would be to include references to the original analysis software: \"A reproducible metabarcoding workflow was implemented in Tapirs [1] using blast [2], Kraken2 [3], and Krona [4].\"","title":"FAQs"},{"location":"About/faqs/#why-is-it-called-tapirs-what-does-it-stand-for","text":"Tapirs is not an acronym, we just called it Tapirs because an easy name helps everyone, and tapirs are cool.","title":"Why is it called Tapirs? What does it stand for?"},{"location":"About/faqs/#why-did-you-make-this-and-not-use-x","text":"Nothing met our needs, see the background page for discussion. We wanted an extensible workflow, built by a specialist workflow manager, that would allow real reproducibility.","title":"Why did you make this and not use X"},{"location":"About/faqs/#can-you-add-in-my-favourite-software-for-me","text":"No, sorry. We really hope however that choosing a workflow manager (Snakemake) means that this is much easier than for any other metabarcoding package. Look at the extending Tapirs How-To guide .","title":"Can you add in my favourite software for me?"},{"location":"About/faqs/#ive-added-some-tools-do-you-want-my-changes","text":"Yes please. Do this via a pull request on github.","title":"I've added some tools, do you want my changes?"},{"location":"About/faqs/#there-is-an-error-in-the-documentation","text":"You should just be able to edit and fix it on Github. Open the page and click the pencil icon, top right, to edit. Thanks for your help.","title":"There is an error in the documentation"},{"location":"About/faqs/#who-do-i-contact-about-the-software","text":"The best way is to flag this on GitHub. If you need to get in touch personally then contact Dave Lunt on Gmail.","title":"Who do I contact about the software?"},{"location":"About/faqs/#is-it-licensed","text":"No, all parts that we have written are free under CC0 public domain and you may do as you wish. Yes, you may include whatever you wish into your own software, we encourage this. All software components that we have included (rather than written, see environment.yaml) have been chosen because they have permissive open source licences, but you may wish to check the details for yourself.","title":"Is it licensed?"},{"location":"About/faqs/#is-there-a-citation","text":"We greatly appreciate you citing us wherever you are able. A suitable citation might be: Title: Tapirs, an extensible workflow for reproducible metabarcoding Authors: URL: https://github.com/davelunt/Tapirs An appropriate way to do this would be to include references to the original analysis software: \"A reproducible metabarcoding workflow was implemented in Tapirs [1] using blast [2], Kraken2 [3], and Krona [4].\"","title":"Is there a citation"},{"location":"How-To-Guide/extending/","text":"Advice on extending the workflow by adding tools, and other approaches. Rationale What is the best approach for your data analysis? Whatever you think is the answer you will need to gather evidence. Workflow systems like snakemake allow relatively easy addition and extension of the analyses. This is very useful in comparative analyses. If a new paper claims taxonomic assignment method Y is better than method X then you may wish to carry out both and compare the results, while standardising the other parts of you workflow. Adding tools Although familiarity with Snakemake is helpful, in many situations you may be able to add a tool by modifying an existing rule. First make sure that your new method produces the results you expect when run at the command line. If it doesn't work at the command line it won't work in Snakemake. Snakemake rules have 3 parts; input, output, and the command to turn the first into the second. Common problems Teaching and trouble-shooting Snakemake and bioinformatics are beyond the scope of this document. A couple of pointers however can save a lot of time. Most problems are because you have a typo If you have multiple lines of input or output each line except the last must finish in a comma. rule test: input: file1: \"firstfile.fasta\", file2: \"secondfile.fasta\" output: \"allseqs.fasta\" shell: \"cat {input.file1} {input.file2} >> {output}\" contributing your improvements to Tapirs We would love to hear from you about the improvements you've made. A pull-request for your git branch would probably be best. Why not help improve the documentation? A new and slightly different tutorial is always welcome.","title":"Extending the Workflow"},{"location":"How-To-Guide/extending/#rationale","text":"What is the best approach for your data analysis? Whatever you think is the answer you will need to gather evidence. Workflow systems like snakemake allow relatively easy addition and extension of the analyses. This is very useful in comparative analyses. If a new paper claims taxonomic assignment method Y is better than method X then you may wish to carry out both and compare the results, while standardising the other parts of you workflow.","title":"Rationale"},{"location":"How-To-Guide/extending/#adding-tools","text":"Although familiarity with Snakemake is helpful, in many situations you may be able to add a tool by modifying an existing rule. First make sure that your new method produces the results you expect when run at the command line. If it doesn't work at the command line it won't work in Snakemake. Snakemake rules have 3 parts; input, output, and the command to turn the first into the second.","title":"Adding tools"},{"location":"How-To-Guide/extending/#common-problems","text":"Teaching and trouble-shooting Snakemake and bioinformatics are beyond the scope of this document. A couple of pointers however can save a lot of time. Most problems are because you have a typo If you have multiple lines of input or output each line except the last must finish in a comma. rule test: input: file1: \"firstfile.fasta\", file2: \"secondfile.fasta\" output: \"allseqs.fasta\" shell: \"cat {input.file1} {input.file2} >> {output}\"","title":"Common problems"},{"location":"How-To-Guide/extending/#contributing-your-improvements-to-tapirs","text":"We would love to hear from you about the improvements you've made. A pull-request for your git branch would probably be best. Why not help improve the documentation? A new and slightly different tutorial is always welcome.","title":"contributing your improvements to Tapirs"},{"location":"How-To-Guide/installation/","text":"Note These documents assume a unix system like OSX or Linux Although you can install and run Tapirs without too many steps you will need some very basic knowledge of the command line. A basic knowledge of Snakemake will help you to modify and configure Tapirs. Snakemake is a relatively easy workflow manager, but we recommend that you familiarise yourself with it, perhaps carry out the tutorial . git clone tapirs If you have git installed then clone the repository to your local environment with git clone github_address You could alternatively download the repository from the Tapirs github repository using the green button. Then expand the zip file and navigate into the directory. install miniconda Conda (miniconda) is a package manager and is required here to install software and their dependencies. Follow the installation instructions for Miniconda for your operating system. create and activate a conda environment Navigate to the tapirs directory. conda create env -f environment.yaml conda activate tapirs The software listed in environment.yaml file should now be installed. You also need to install BASTA, KronaTools, and a series of databases for blastn, BASTA, Krona, and Kraken2. Databases are large files, and everyone needs a different one, so they are not included with this install. BASTA can be problematic to install and requires its own python environment separate from the rest of the workflow. create a second environment, for BASTA Warning do we need to do this or will snakemake take care of creating the environment? The LCA analysis software BASTA requires a different version of python (2.7) and needs to be run in its own environment. Snakemake will take of this but you should first create it from the basta_env.yaml file. conda create env -f envs/basta_env.yaml","title":"Installation"},{"location":"How-To-Guide/installation/#git-clone-tapirs","text":"If you have git installed then clone the repository to your local environment with git clone github_address You could alternatively download the repository from the Tapirs github repository using the green button. Then expand the zip file and navigate into the directory.","title":"git clone tapirs"},{"location":"How-To-Guide/installation/#install-miniconda","text":"Conda (miniconda) is a package manager and is required here to install software and their dependencies. Follow the installation instructions for Miniconda for your operating system.","title":"install miniconda"},{"location":"How-To-Guide/installation/#create-and-activate-a-conda-environment","text":"Navigate to the tapirs directory. conda create env -f environment.yaml conda activate tapirs The software listed in environment.yaml file should now be installed. You also need to install BASTA, KronaTools, and a series of databases for blastn, BASTA, Krona, and Kraken2. Databases are large files, and everyone needs a different one, so they are not included with this install. BASTA can be problematic to install and requires its own python environment separate from the rest of the workflow.","title":"create and activate a conda environment"},{"location":"How-To-Guide/installation/#create-a-second-environment-for-basta","text":"Warning do we need to do this or will snakemake take care of creating the environment? The LCA analysis software BASTA requires a different version of python (2.7) and needs to be run in its own environment. Snakemake will take of this but you should first create it from the basta_env.yaml file. conda create env -f envs/basta_env.yaml","title":"create a second environment, for BASTA"},{"location":"How-To-Guide/reproducibility/","text":"Reproducible Analyses Reproducibility is important. The main person it is helping is you. Exit Strategy When an experiment is finished you should have an 'exit strategy' checklist to make sure your work is as reproducible as possible. We hope that we have made this achievable in Tapirs - data provinance + data archive is possible - list of all software, sources, and versions + conda export of environment, all software conda installable - workflow - human readable reports - easy to archive Software list and versions The full list of software, their dependencies and version numbers called environment.yaml is written to envs/ directory at the end of the run. This file can be used to reproduce the experimental software conditions. Archiving Snakemake can be asked to make an archive of code, cofig, and input file: snakemake --archive my-workflow.tar.gz Transparent workflow","title":"Reproducibiility"},{"location":"How-To-Guide/reproducibility/#reproducible-analyses","text":"Reproducibility is important. The main person it is helping is you.","title":"Reproducible Analyses"},{"location":"How-To-Guide/reproducibility/#exit-strategy","text":"When an experiment is finished you should have an 'exit strategy' checklist to make sure your work is as reproducible as possible. We hope that we have made this achievable in Tapirs - data provinance + data archive is possible - list of all software, sources, and versions + conda export of environment, all software conda installable - workflow - human readable reports - easy to archive","title":"Exit Strategy"},{"location":"How-To-Guide/reproducibility/#software-list-and-versions","text":"The full list of software, their dependencies and version numbers called environment.yaml is written to envs/ directory at the end of the run. This file can be used to reproduce the experimental software conditions.","title":"Software list and versions"},{"location":"How-To-Guide/reproducibility/#archiving","text":"Snakemake can be asked to make an archive of code, cofig, and input file: snakemake --archive my-workflow.tar.gz Transparent workflow","title":"Archiving"},{"location":"How-To-Guide/setup/","text":"install first then setup These instructions are to set up Tapirs after installation has been carried out. CONFIG FILES The config.yaml file contains settings for the workflow's operation. Most of them have reasonable default values, but some (like specifying the location of your data) require your input. Open config.yaml in a text editor. set a name for your experiment (default=expt_name) set the directory containing your demultiplexed fastq.gz data (default=data/demultiplexed) set the location of your databases (ignore if not using the program) blast database Kraken2 database set fastp parameters for quality control and read merging set blast parameters DATA The /data directory should contain 2 subdirectories /demultiplexed /databases demultiplexed The start point of this workflows is demultiplexed fastq.gz sequence files in the data/demultiplexed directory. There are many ways to produce these files, and your sequencing machine or sequencing centre will most likely return this as the default data. It is essential for reproducibility that this starting data is kept together with the experiment. If size prohibits its inclusion then archive it publicly and include the doi into the experimental record. tip: exclude data directory from version control Remember that if you are wisely keeping your analysis under git version control, then you can exclude very large data files from syncing to the web by including the data directory in your .gitignore file. It is essential however that you make other arrangements for both backing up and publishing the data. Databases Kraken2 In order to run a Kraken analysis you will need to create a Kraken database for the program to search your query sequences against. These databases can be large and require a lot of RAM and time to build them. They only need to be made once however and then all subsequent analyses can be performed against the same database. If you are building a custom database from a few thousand sequences then database construction will likely be quick. See the Kraken Tutorial and the Kraken2 manual for information on building Kraken databases. create a Kraken database If you have a single fasta format file (allseqs.fas) containing all the sequences to be included in the database, then you could create a kraken database with the command: database creation example here It is essential for reproducibility that you publicly archive your database at the end of the experiment. Zenodo.org is a suitable location. blast You will need to build a blastn database from a collection of fasta files. Information on this can be found at the NCBI site Blast help pages . create a blast database If you have a single fasta format file (allseqs.fas) containing all the sequences to be included in the database, then you could create a blast database with the command: database creation example here See the Blast Tutorial for more detailed help It is essential for reproducibility that you publicly archive your database at the end of the experiment. Zenodo.org is a suitable location. taxonomy database Several programs require the NCBI taxonomy database in order to carry out taxonomic assignment (BASTA, Kraken). DRY RUN SNAKEFILE Make sure you are in the directory containing the snakefile then type snakefile -npr to dry-run the workflow. If all has gone well Snakemake will report the jobs it needs to perform without any complaint. If not (and it is the usual situation) you will need to diagnose and fix any minor issues.","title":"Setup Tapirs"},{"location":"How-To-Guide/setup/#config-files","text":"The config.yaml file contains settings for the workflow's operation. Most of them have reasonable default values, but some (like specifying the location of your data) require your input. Open config.yaml in a text editor. set a name for your experiment (default=expt_name) set the directory containing your demultiplexed fastq.gz data (default=data/demultiplexed) set the location of your databases (ignore if not using the program) blast database Kraken2 database set fastp parameters for quality control and read merging set blast parameters","title":"CONFIG FILES"},{"location":"How-To-Guide/setup/#data","text":"The /data directory should contain 2 subdirectories /demultiplexed /databases","title":"DATA"},{"location":"How-To-Guide/setup/#demultiplexed","text":"The start point of this workflows is demultiplexed fastq.gz sequence files in the data/demultiplexed directory. There are many ways to produce these files, and your sequencing machine or sequencing centre will most likely return this as the default data. It is essential for reproducibility that this starting data is kept together with the experiment. If size prohibits its inclusion then archive it publicly and include the doi into the experimental record. tip: exclude data directory from version control Remember that if you are wisely keeping your analysis under git version control, then you can exclude very large data files from syncing to the web by including the data directory in your .gitignore file. It is essential however that you make other arrangements for both backing up and publishing the data.","title":"demultiplexed"},{"location":"How-To-Guide/setup/#databases","text":"","title":"Databases"},{"location":"How-To-Guide/setup/#kraken2","text":"In order to run a Kraken analysis you will need to create a Kraken database for the program to search your query sequences against. These databases can be large and require a lot of RAM and time to build them. They only need to be made once however and then all subsequent analyses can be performed against the same database. If you are building a custom database from a few thousand sequences then database construction will likely be quick. See the Kraken Tutorial and the Kraken2 manual for information on building Kraken databases. create a Kraken database If you have a single fasta format file (allseqs.fas) containing all the sequences to be included in the database, then you could create a kraken database with the command: database creation example here It is essential for reproducibility that you publicly archive your database at the end of the experiment. Zenodo.org is a suitable location.","title":"Kraken2"},{"location":"How-To-Guide/setup/#blast","text":"You will need to build a blastn database from a collection of fasta files. Information on this can be found at the NCBI site Blast help pages . create a blast database If you have a single fasta format file (allseqs.fas) containing all the sequences to be included in the database, then you could create a blast database with the command: database creation example here See the Blast Tutorial for more detailed help It is essential for reproducibility that you publicly archive your database at the end of the experiment. Zenodo.org is a suitable location.","title":"blast"},{"location":"How-To-Guide/setup/#taxonomy-database","text":"Several programs require the NCBI taxonomy database in order to carry out taxonomic assignment (BASTA, Kraken).","title":"taxonomy database"},{"location":"How-To-Guide/setup/#dry-run-snakefile","text":"Make sure you are in the directory containing the snakefile then type snakefile -npr to dry-run the workflow. If all has gone well Snakemake will report the jobs it needs to perform without any complaint. If not (and it is the usual situation) you will need to diagnose and fix any minor issues.","title":"DRY RUN SNAKEFILE"},{"location":"Tutorials/blast_tutorial/","text":"Basic blast tutorial get reference data build blast database get query sequences blast lca outputs","title":"Basic blast"},{"location":"Tutorials/blast_tutorial/#basic-blast-tutorial","text":"get reference data build blast database get query sequences blast lca outputs","title":"Basic blast tutorial"},{"location":"Tutorials/kraken_tutorial/","text":"Kraken databases Kraken output formats","title":"Kraken2"},{"location":"Tutorials/kraken_tutorial/#kraken-databases","text":"","title":"Kraken databases"},{"location":"Tutorials/kraken_tutorial/#kraken-output-formats","text":"","title":"Kraken output formats"}]}